{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression & Numerical Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea\n",
    "\n",
    "Get the probability of a sample belonging to a class.\n",
    "\n",
    "- regression task used for classification: \n",
    "    - the output is the probability of a sample belonging to a class.\n",
    "    - the output is transformed to a binary value using a threshold.\n",
    "- useful if we want to prioritize the most likely class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Class = Logistic Function\n",
    "\n",
    "Transforms an observation ($x$) into a probability using the function: \n",
    "\n",
    "$$\n",
    "    H(x) = \\frac{1}{1 + e^{-\\langle \\purple{w}, x \\rangle}}\n",
    "$$\n",
    "\n",
    "- the \"Logistic Function\"\n",
    "- Parameterized by $w$.\n",
    "- The probability that we label $x$ as $1$\n",
    "\n",
    "**Note:** \n",
    "The function is **monotonic** (always increasing). So, the probability of a sample belonging to a class increases as the dot product of the weights and the sample increases.\n",
    "\n",
    "\n",
    "#### As a stochastic model\n",
    "\n",
    "$$\n",
    "    H(X) = P(Y=1 | X,\\purple{w})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"container\" style=\"display: flex\">\n",
    "    <img src=\"img/IMG_DF2CD5C31F78-1.jpeg\" style=\"width: 25%; margin-right:2%\"></img>\n",
    "    <div>\n",
    "        As you can see, using a linear model (i.e. linear regression) is not appropiate.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sigmoid function:\n",
    "\n",
    "def sigmoid(x, a, b):\n",
    "    return 1 / (1 + np.exp(-a * (x - b)))\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(x, 1, 0)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.yticks([0,0.5, 1])\n",
    "plt.xticks([])\n",
    "plt.xlabel('<w,x>')\n",
    "plt.ylabel('h(x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Function\n",
    "\n",
    "def getLogisticFunction(w: np.ndarray):\n",
    "    return lambda x: 1 / (1 + np.exp(np.dot(w,x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Parameters, $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA: View Likelihood Function in terms of a Bernoulli Variable\n",
    "\n",
    "<div id=\"container\" style=\"display: flex\">\n",
    "    <img src=\"img/img2.jpeg\" style=\"width: 25%; margin-right:2%\"></img>\n",
    "    <div>\n",
    "        <ul>\n",
    "            <li>The probability of a sample belonging to a class is modeled as a Bernoulli variable.</li>\n",
    "            <li>p changes depending on where the sample is in the feature space (ie: p=H(x))</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using the bernoulli RV\n",
    "\n",
    "We model the outcome, $y$ as a Bernoulli RV: \n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        L(\\pink{p} | y) \n",
    "        &= \n",
    "        \\prod_{i=1}^n \\pink{p}^{y_i} (1 - \\pink{p})^{1-y_i}\n",
    "        \\\\\n",
    "        &= \n",
    "        \\begin{cases} \n",
    "            \\prod_{i=1}^n \\pink{p} & \\text{for } y_i = 1 \\\\\n",
    "            \\prod_{i=1}^n (1 - \\pink{p}) & \\text{for } y = 0\n",
    "        \\end{cases}\n",
    "    \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Log Likelihood\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\log L(\\pink{p} | y) \n",
    "        &= \n",
    "        \\sum_{i=1}^n \\log \\pink{p}^{y_i} + (1 - \\pink{p})^{1 - y_i}\n",
    "        \\\\\n",
    "        &=\n",
    "        \\sum_{i=1}^n y_i \\log \\pink{p} + \\log (1 - y_i)(1 - \\pink{p})\n",
    "        \\\\\n",
    "        &=\n",
    "        \\begin{cases} \n",
    "            \\sum_{i=1}^n \\log \\pink{p}\n",
    "            & \\text{for } y_i = 1 \n",
    "            \\\\\n",
    "            \\sum_{i=1}^n \\log (1 - \\pink{p})\n",
    "            & \\text{for } y = 0\n",
    "        \\end{cases}\n",
    "    \\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Using our Logistic Function \n",
    "\n",
    "i.e. $p = H(x)=P[Y=1 | x,w]$\n",
    "\n",
    "$$\n",
    "        \\log L(\\pink{p} | y) \n",
    "        = \n",
    "        \\begin{cases} \n",
    "                \\sum_{i=1}^n \\log(\\pink{1-\\frac{1}{1+e^{-\\langle \\purple{w}, x \\rangle}}})\n",
    "                & \\text{for } y_i = 1 \n",
    "                \\\\\n",
    "                \\sum_{i=1}^n  \\log (1 - (\\pink{1-\\frac{1}{1+e^{-\\langle \\purple{w}, x \\rangle}}}))\n",
    "                & \\text{for } y = 0\n",
    "        \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "        \\log L(\\purple{w} | x,y)\n",
    "        =\n",
    "        \\begin{cases} \n",
    "                \\sum_{i=1}^n - \\log(1 + e^{- \\langle \\purple{w}, x \\rangle})\n",
    "                & \\text{for } y_i = 1 \n",
    "                \\\\\n",
    "                \\sum_{i=1}^n  - \\log (1 + e^{\\langle \\purple{w}, x \\rangle}))\n",
    "                & \\text{for } y = 0\n",
    "        \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use function for $y$'s sign\n",
    "\n",
    "Let \n",
    "\n",
    "\n",
    "$$\n",
    "\\tilde{y}_i = \n",
    "    \\begin{cases}\n",
    "        1  & \\text{for } y_i = 1 \\\\\n",
    "        -1 & \\text{for } y_i = 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Then: \n",
    "\n",
    "$$\n",
    "        \\log L(\\purple{w} | x,y)\n",
    "        =\n",
    "        \\sum_{i=1}^n - \\log(1 + e^{-\\tilde{y}_i \\langle \\purple{w}, x \\rangle})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find $w$ that maximizes the likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\hat{\\purple{w}}\n",
    "        &= \n",
    "        \\text{argmax}_{\\purple{w}} \\sum_{i=1}^n - \\log(1 + e^{- \\tilde{y}_i \\langle \\purple{w}, x \\rangle})\n",
    "        \\\\\n",
    "        &=\n",
    "        \\text{argmin}_{\\purple{w}} \\sum_{i=1}^n \\log(1 + e^{- \\tilde{y}_i \\langle \\purple{w}, x \\rangle})\n",
    "        \\\\\n",
    "        &=\n",
    "        \\text{argmin}_{\\purple{w}} \\frac{1}{n} \\sum_{i=1}^n \\log(1 + e^{- \\tilde{y}_i \\langle \\purple{w}, x \\rangle})\n",
    "        \n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "Get the expected value: \n",
    "\n",
    "$$\n",
    "    E[l_{\\purple{w}}(x_i, y_i)] = \\frac{1}{n} \\sum_{i=1}^n \\log(1 + e^{- \\tilde{y}_i \\langle \\purple{w}, x \\rangle})\n",
    "$$\n",
    "\n",
    "**Cross Entropy Loss:**\n",
    "\n",
    "\"Cross entropy\" means we mean is that we are comparing the predicted probability $y$ with the true probability ($\\tilde{y}$)\n",
    "\n",
    "This is what we are trying to minimize.\n",
    "\n",
    "$$\n",
    "    l_{\\purple{w}}(x, \\tilde{y}) = \\log(1 + e^{- \\tilde{y}_i \\langle \\purple{w}, x \\rangle})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: \n",
    "\n",
    "<img src=\"img/img3.jpeg\" style=\"width:50%\" ></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach #1: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall:** *Gradient* is the direction of the steepest ascent (vector of partial derivatives). \n",
    "\n",
    "**Idea:** Move in the opposite direction of the gradient to minimize the loss. \n",
    "\n",
    "Let $\\eta =$ step size, then: \n",
    "\n",
    "$$\n",
    "    w_t = w_{t-1} - \\eta \\nabla f(w_{t-1})\n",
    "$$\n",
    "\n",
    "For the loss function, this is computed over a batch of $n$ samples: \n",
    "\n",
    "$$\n",
    "    \\nabla l_{w_{t-1}}(x, \\tilde{y}) = \\frac{1}{n} \\sum_{i=1}^n \\nabla l_{w_{t-1}}(x_i, \\tilde{y}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach #2: Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:** Use the second derivative to get a better approximation of the minimum.\n",
    "\n",
    "Newton's Method finds the roots of $f(w)$ by iteratively updating $w$ using the following formula:\n",
    "\n",
    "<img src=\"img/img4.jpeg\" style=\"width: 40%\"/>\n",
    "\n",
    "In parameter estimation, we want the roots of $l'_w(x)$:\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        w_1 & = w_0-\\frac{l'_w(x)}{l''_w(x)}\n",
    "        \\\\ &= \n",
    "        w_0 - \\frac{\\nabla l_w(x)}{\\nabla^2 l_w(x)}\n",
    "    \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Regression\n",
    "\n",
    "#### *(Generalizing to Multi Class)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Softmax** Function. For $k$ classes:\n",
    "\n",
    "$$\n",
    "    Pr[y=k | x,w] = \\frac\n",
    "        {e^{\\langle w_k, x\\rangle}}\n",
    "        {\\sum_{i=1}^k e^{\\langle w_i, x\\rangle}}\n",
    "$$\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <div>\n",
    "        <h3>Binary</h3>\n",
    "        <img src=\"./img/img5.png\" style=\"max-width: 50%; object-fit: contain;\"/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h3>Multiclass</h3>\n",
    "        <img src=\"./img/img6.png\" style=\"max-width: 50%; object-fit: contain;\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
