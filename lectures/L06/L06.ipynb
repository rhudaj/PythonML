{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Margin SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA #1 (DATAFRAME D1)\n",
    "\n",
    "# Create some data with 100 samples, 2 features, 1 class label, that's linearly separable:\n",
    "n = 50\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(n, 2)\n",
    "y = np.random.choice([-1, 1], n)\n",
    "\n",
    "# Shift the classes to make them linearly separable with a clear gap\n",
    "X[y == 1] += 2\n",
    "X[y == -1] -= 2\n",
    "\n",
    "# Create a DataFrame with features and class labels:\n",
    "D1 = pd.DataFrame(X, columns=['x1', 'x2'])\n",
    "D1['class'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea: \n",
    "\n",
    "**WHAT**\n",
    "\n",
    "SVM finds the hyperplane that <u>best</u> separates the data into two classes.\n",
    "- <u>best</u> = maximizes the margin between the two classes.\n",
    "\n",
    "**WHY**\n",
    "\n",
    "Having a large margin means that the model is less likely to overfit the data.\n",
    "\n",
    "<img src=\"img/img1.jpeg\" style=\"width: 50%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Distance between a point and Hyperplane \n",
    "\n",
    "Given: \n",
    "\n",
    "- $w^T x = -b$\n",
    "- any point on the hyperplane: $x_h$\n",
    "- Point of interest: $x_i$ \n",
    "\n",
    "Let: \n",
    "\n",
    "- $v = x_i - x_h$ \n",
    "\n",
    "Min distance: \n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        d \n",
    "        &= \\frac{w}{||w||} \\cdot v\n",
    "        \\\\\n",
    "        &= \\frac{w^T x_i + b}{||w||}\n",
    "\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"img/img5.jpeg\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO: MIN DISTANCE BETWEEN POINTS AND HYPERPLANE\n",
    "\n",
    "def signedDistPointToLine(w, b, x):\n",
    "    return (np.dot(x, w) + b) / np.linalg.norm(w)\n",
    "\n",
    "# now find the point with the smallest distance to the decision boundary:\n",
    "def find_closest_point(w, b, X):\n",
    "    # calculate the distance from each point to the decision boundary\n",
    "    signed_dists = [ signedDistPointToLine(w, b, x) for x in X ]\n",
    "    # find the point with the smallest distance\n",
    "    i = np.argmin(np.abs(signed_dists))\n",
    "    return X[i], signed_dists[i]\n",
    "\n",
    "# PLOTTING:\n",
    "\n",
    "# Now that the data is created, create a decision boundary and plot it:\n",
    "def plot_decision_boundary(w, b, D: pd.DataFrame):\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = (-w[0] * x - b) / w[1]\n",
    "    plt.plot(x, y, 'k--')\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    # plot the data: use first 2 columns of D as x and y, and color by class:\n",
    "    plt.scatter(D['x1'], D['x2'], c=D['class'], cmap='coolwarm')\n",
    "\n",
    "\n",
    "def closestPointFromDecisionBoundary(w1=1, w2=1, b: int=0):\n",
    "    w = [w1, w2]\n",
    "    cp, signed_dist = find_closest_point(w, b, D1.iloc[:, :2].values)\n",
    "\n",
    "    v = ( w / np.linalg.norm(w) ) * -signed_dist    # vector between closest point and decision boundary\n",
    "    pol = cp + v                            # point on the decision boundary\n",
    "\n",
    "    # PLOT: data\n",
    "    plt.scatter(D1['x1'], D1['x2'], c=D1['class'], cmap='coolwarm')\n",
    "    # PLOT: decision boundary\n",
    "    plot_decision_boundary(w, b, D1)\n",
    "    # PLOT: closest point to the line\n",
    "    plt.scatter(cp[0], cp[1], c='black', s=100)\n",
    "    # PLOT: line btwn cp & pol:\n",
    "    plt.plot([pol[0], cp[0]], [pol[1], cp[1]], 'k--')\n",
    "\n",
    "\n",
    "interact (closestPointFromDecisionBoundary, w1=(-3, 3, 0.1), w2=(-3, 3, 0.1), b=(-2,2,0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Margin \n",
    "\n",
    "\"Margin\" = distance from hyperplane (decision boundary) to the closest point. \n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\gamma \n",
    "        &= \\text{min}_i( y_i \\frac{w^T x_i + b}{||w||})\n",
    "        \\\\\n",
    "        &= \\frac{1}{||w||} \\text{min}_i( y_i (w^T x_i + b))\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "- $y_i \\in [-1, 1]$ (the class) converts the distance to be unsigned.\n",
    "\n",
    "**Diagram:**\n",
    "\n",
    "<img src=\"img/img2.jpeg\" style=\"width: 30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Max Margin\" Classifier Problem \n",
    "\n",
    "We need to maximize the margin *(the min distance)*.\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\hat{w}, \\hat{b} \n",
    "        &= \\text{argmax}_{w,b} \\gamma\n",
    "        \\\\\n",
    "        &= \\text{argmax}_{w,b} ( \n",
    "            \\frac{1}{||w||} \\text{min}_i( y_i (w^T x_i + b))\n",
    "        )\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "<img src=\"img/img3.jpeg\" style=\"width: 50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO: MARGIN\n",
    "\n",
    "def marginFromDecisionBoundary(w1=1, w2=1, b: int=0):\n",
    "    w = [w1, w2]\n",
    "    cp, signed_dist = find_closest_point(w, b, D1.iloc[:, :2].values)\n",
    "    margin = np.abs(signed_dist)\n",
    "\n",
    "    # PLOT: data\n",
    "    plt.scatter(D1['x1'], D1['x2'], c=D1['class'], cmap='coolwarm')\n",
    "\n",
    "    # PLOT: decision boundary\n",
    "    plot_decision_boundary(w, b, D1)\n",
    "\n",
    "    # PLOT: closest point to the line\n",
    "    plt.scatter(cp[0], cp[1], c='black', s=100)\n",
    "\n",
    "    # PLOT: margin\n",
    "    x_fill = np.arange(-3, 3, 0.01)\n",
    "    y_fill =  - (w[0]*x_fill + b) / w[1]    # y-values on decision boundary\n",
    "    plt.fill_between(x_fill, y_fill - margin, y_fill + margin, color='grey', alpha=0.5)\n",
    "\n",
    "\n",
    "interact (marginFromDecisionBoundary, w1=(-3, 3, 0.1), w2=(-3, 3, 0.1), b=(-2,2,0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Parameters does not change the Decision Boundary\n",
    "\n",
    "$w \\rarr \\alpha w$, $b \\rarr \\alpha b$\n",
    "\n",
    "For each $x_i$, the decision boundary intersects the $x_i$-axis at $x_i = -\\frac{b}{w_i} = -\\frac{\\alpha \\cdot b}{\\alpha \\cdot w_i}$. \n",
    "\n",
    "\n",
    "The distance between any point and the boundary does not change either.\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        d \n",
    "        &= \n",
    "        \\frac{w^T x_i + b}{||w||}\n",
    "        \\\\\n",
    "        &=\n",
    "        \\frac{\\red{\\alpha} w^T x_i + \\red{\\alpha}b}{||\\red{\\alpha}w||}\n",
    "        \\\\\n",
    "        &=\n",
    "        \\frac{\\red{\\alpha}}{\\red{\\alpha}}\n",
    "        \\frac\n",
    "            { w^T x_i + b}\n",
    "            {||w||}\n",
    "    \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO: Scaling Parameters\n",
    "\n",
    "def scaleParameters(scale=1):\n",
    "    w=[1,1]\n",
    "    b=1\n",
    "    marginFromDecisionBoundary(w1=w[0]*scale, w2=w[1]*scale, b=-b*scale)\n",
    "    plt.legend([\n",
    "        f'x1: -{scale}*b / {scale}*w1 = {-b/w[0]}',\n",
    "        f'x2: -{scale}*b / {scale}*w2 = {-b/w[1]}',\n",
    "    ])\n",
    "\n",
    "interact (scaleParameters, scale=(1, 10, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restating the Objective (Constrained Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 equations: \n",
    "\n",
    "1. Positive class: $w^T x_i + b = 1$\n",
    "2. Negative class: $w^T x_i + b = -1$\n",
    "\n",
    "i.e. Rescale the equation to have the margin = +1/-1\n",
    "\n",
    "<img src=\"img/img4.jpeg\" style=\"width: 40%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn our nested (max over min) optimization problem into a single (constrained) optimization problem (easier to compute):\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\hat{w}, \\hat{b} \n",
    "        &= \\text{argmax}_{w,b} \\gamma\n",
    "        \\\\\n",
    "        &= \\text{argmax}_{w,b} \n",
    "            \\frac{1}{||w||} \\text{min}_i y_i (w^T x_i + b)\n",
    "        \\\\\n",
    "        &= \\text{argmax}_{w,b} \n",
    "            \\frac{1}{||w||} \n",
    "            , \\ \\ \\ \\ \\  \\text{subject to: }  y_i (w^T x_i + b) \\ge 1\n",
    "        \\\\\n",
    "        &= \\text{argmin}_{w,b} \n",
    "            \\frac{1}{2} ||w||^2\n",
    "             , \\ \\ \\ \\text{subject to: }  y_i (w^T x_i + b) \\ge 1\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note:** We take half the squared weight to make the math easier later on. We can do this since the margin is always positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Constraint Optimization Problem\n",
    "### <span style=\"color: grey;\">Using Lagrange Multipliers (the Dual Problem)</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrangian Dual Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual problem helps solve constrained problems by transforming them into a new form.\n",
    "\n",
    "The Lagrangian $L(x,\\lambda)$ is a function that incorporates both the objective function and the constraint:\n",
    "\n",
    "$$\n",
    "    L(x, \\lambda) = f(x) - \\lambda (x - c)\n",
    "$$\n",
    "\n",
    "where $\\lambda\\ge 0$ is the Lagrange multiplier that enforces the constraint.\n",
    "\n",
    "\n",
    "The dual problem involves:\n",
    "\n",
    "$$\n",
    "        \\text{min}_x\n",
    "        \\text{max}_{\\lambda}\n",
    "        L(x, \\lambda) \n",
    "$$\n",
    "\n",
    "1. Minimizing  $L(x,\\lambda)$ with respect to $x$\n",
    "2. Maximizing it with respect to $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize: $f(x)=0.2x^2-x+1$, subject to $x \\ge \\red{5}$\n",
    "\n",
    "Let: \n",
    "\n",
    "- $L(x, \\lambda) = f(x) - \\lambda (x-\\red{5})$\n",
    "- $\\lambda \\ge 0$\n",
    "\n",
    "---\n",
    "\n",
    "1. Minimize over $x$:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial x} = 0.4x - 1 - \\lambda\n",
    "$$\n",
    "\n",
    "Let: \n",
    "\n",
    "$$\n",
    "    0.4x - 1 - \\lambda = 0\n",
    "$$\n",
    "\n",
    "2. Maximize over $\\lambda$:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial \\lambda} = x - 5\n",
    "$$\n",
    "\n",
    "Let: \n",
    "\n",
    "$$\n",
    "    x - 5 = 0\n",
    "    \\\\\n",
    "    \\implies x = 5\n",
    "    \n",
    "$$\n",
    "\n",
    "\n",
    "3. Combine: \n",
    "\n",
    "$$\n",
    "    \\lambda = 0.4 (5) - 1 = 1\n",
    "$$\n",
    "\n",
    "\n",
    "As $\\lambda \\ge 0$, $x=5$ is a valid solution.\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO: Solving the Dual problem\n",
    "\n",
    "x = np.arange(-5, 10, 0.1)\n",
    "y = 0.2*(x**2) - x + 1\n",
    "\n",
    "def findMin(x: np.ndarray, y: np.ndarray, x_min: int):\n",
    "    i = np.argmax(x >= x_min)\n",
    "    i_min = np.argmin(y[i:])\n",
    "    return x[i + i_min], y[i + i_min]\n",
    "\n",
    "def getLagrangian(x: np.ndarray, y:np.ndarray, minX: int, LM: int):\n",
    "    return y - (LM*(x-minX))\n",
    "\n",
    "\n",
    "def plotDualProblem(y: np.ndarray, minX: int, LM=1):\n",
    "    L = y - (LM*(x-minX))\n",
    "    plt.plot(x, y)\n",
    "    plt.plot(x,L)\n",
    "    plt.vlines(minX, -5, 10, colors='black', linestyles='dashed')\n",
    "    # ensure that the x-axis is fixed:\n",
    "    plt.xlim(-5, 10)\n",
    "    plt.ylim(-5, 10)\n",
    "    # get the min point on y:\n",
    "    min_x, min_y = findMin(x=x, y=y, x_min=minX)\n",
    "    plt.scatter(min_x, min_y, c='black', s=100)\n",
    "\n",
    "\n",
    "interact (plotDualProblem, y=fixed(y), minX=(-5, 10, 0.1), LM=(0, 10, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x)=0.2x^2-x+1$, subject to $x \\ge \\red{-3}$\n",
    "\n",
    "=> \n",
    "\n",
    "- $L(x, \\lambda) = f(x) - \\lambda (x+\\red{3})$\n",
    "- $\\lambda \\ge 0$\n",
    "\n",
    "---\n",
    "\n",
    "1. Minimize over $x$:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial x} = 0.4x - 1 - \\lambda\n",
    "$$\n",
    "\n",
    "Let: \n",
    "\n",
    "$$\n",
    "    0.4x - 1 - \\lambda = 0\n",
    "$$\n",
    "\n",
    "2. Maximize over $\\lambda$:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial \\lambda} = x - 5\n",
    "$$\n",
    "\n",
    "Let: \n",
    "\n",
    "$$\n",
    "    x +3 = 0\n",
    "    \\\\\n",
    "    \\implies x = -3\n",
    "    \n",
    "$$\n",
    "\n",
    "\n",
    "3. Combine: \n",
    "\n",
    "$$\n",
    "    \\lambda = 0.4 (-3) - 1 = -2.2\n",
    "$$\n",
    "\n",
    "\n",
    "As $\\lambda \\le 0$, $x=-3$ is NOT a valid solution.\n",
    "\n",
    "\n",
    "4. Let $\\lambda = 0$, solve for $x$:\n",
    "\n",
    "$$\n",
    "    x=\\frac{1}{0.4}=2.5\n",
    "$$\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the dual problem provides insight into both the optimal value of the variable `x` in the original problem and how \\\"binding\\\" the constraint is, represented by the Lagrange multiplier `λ`. Here’s how to intuitively connect the dual solution to the original problem\n",
    "\n",
    "#### 1. **Understanding `x = 5`** (from the dual problem):\n",
    "\n",
    "- The constraint in the original problem is `x ≥ 5`.\n",
    "- The fact that the dual solution gives `x = 5` means that the constraint is **active** or **binding**. In other words, the solution occurs right at the boundary of the constraint, so `x` cannot be less than 5.\n",
    "- Had the solution been `x > 5`, the constraint wouldn’t be binding, meaning the constraint wouldn’t have played a limiting role in the optimization.\n",
    "\n",
    "#### 2. **Understanding `λ = 1`**:\n",
    "\n",
    "- The Lagrange multiplier `λ = 1` gives the **sensitivity** of the objective function to changes in the constraint. Specifically, `λ` measures how much the optimal value of the objective function (here, `f(x)`) would increase if the constraint `x ≥ 5` were relaxed by a small amount.\n",
    "- In this case, `λ = 1` means that if you relaxed the constraint by allowing `x` to be slightly smaller than 5 (e.g., `x ≥ 4.99`), the value of the objective function would improve (decrease) by 1 unit for each unit of relaxation.\n",
    "\n",
    "#### 3. **Interpretation in the Original Problem**:\n",
    "\n",
    "- **Solution for `x`**: From the dual problem, `x = 5` is the value of `x` that minimizes `f(x)` subject to the constraint. This directly answers the original problem: the optimal value of `x` is 5.\n",
    "- **Meaning of `λ`**: `λ = 1` tells us that the constraint `x ≥ 5` is important because relaxing it would improve the objective function. If `λ = 0`, it would mean the constraint wasn’t binding, and the optimization could have proceeded without regard to the constraint.\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "- The dual solution `x = 5` confirms that the optimal solution to the original problem occurs at the boundary of the constraint.\n",
    "- `λ = 1` indicates how strongly the constraint `x ≥ 5` influences the solution—relaxing the constraint would decrease the function value (making the optimization easier).\n",
    "\n",
    "In essence, the dual problem not only provides the solution `x = 5` but also quantifies the \\\"cost\\\" or impact of the constraint via `λ = 1`. You use this information to conclude that the best solution for the original problem is at `x = 5`, and the constraint is crucial to this outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Margin Problem Using Lagrangian Dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{w}, \\hat{b} \n",
    "= \n",
    "\\text{argmin}_{w,b} \n",
    "\\frac{1}{2} ||w||^2\n",
    ", \\ \\ \\ \n",
    "\\text{subject to: }  y_i (w^T x_i + b) \\ge 1\n",
    "$$\n",
    "\n",
    "Notice, the constraint is for every point in the dataset. \n",
    "\n",
    "becomes... \n",
    "\n",
    "$$\n",
    "\\text{min}_{w,b}\n",
    "\\text{max}_{\\blue{\\lambda} \\ge 0}\n",
    "L(w,b,\\blue{\\lambda})\n",
    "= \n",
    "\\frac{1}{2} ||w||^2 - \\sum_i \\blue{\\lambda}_i (y_i (w^T x_i + b) - 1)\n",
    "$$\n",
    "\n",
    "\n",
    "Intuition: \n",
    "\n",
    "- The points that are closest to the decision boundary will have the highest $\\lambda_i$ values (& vice-versa).\n",
    "- Their lambda values will active; used to update the weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Lagrangian Dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    \\text{min}_{w,b}\n",
    "    \\text{max}_{\\blue{\\lambda} \\ge 0}\n",
    "    L(w,b,\\blue{\\lambda})\n",
    "    \n",
    "    &= \n",
    "    \\frac{1}{2} ||w||^2 - \\sum_i \\blue{\\lambda}_i (y_i (w^T x_i + b) - 1)\n",
    "\n",
    "    \\\\ &= \n",
    "    \\frac{1}{2} ||w||^2 - \\sum_i \\blue{\\lambda}_i y_i w^T x_i - \\sum_i \\blue{\\lambda}_i y_i b + \\sum_i \\blue{\\lambda}_i\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "1. Minimize over $w$ and $b$:\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial L}{\\partial w} \n",
    "        &= w - \\sum_i \\blue{\\lambda}_i y_i x_i = 0\n",
    "        \\\\\n",
    "        &\\implies w = \\sum_i \\blue{\\lambda}_i y_i x_i\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "- Note: any point that is not on the margin will have $\\lambda_i = 0$, and those points will not contribute to the weight update.\n",
    "    - => not all data points are needed to train the model.\n",
    "\n",
    "2. Minimize over $b$:\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial L}{\\partial b} \n",
    "        &= - \\sum_i \\blue{\\lambda}_i y_i = 0\n",
    "        \\\\\n",
    "        &\\implies \\sum_i \\blue{\\lambda}_i y_i = 0\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "3. Solve for $\\lambda_i$:\n",
    "\n",
    "<img src=\"img/img6.jpeg\" style=\"width:50%\"/>\n",
    "\n",
    "\n",
    "4. Solve for $b$: \n",
    "\n",
    "$$\n",
    "    y_i(w^T x_i+b) = 1, \\ \\ \\ x_i \\in S_v\n",
    "    \\\\\n",
    "    \\begin{aligned}\n",
    "        \\implies  b &= \\frac{1}{y_i} - w^T x_i\n",
    "        \\\\ &= \n",
    "        y_i - w^T x_i\n",
    "        \\\\ &\\approx\n",
    "        \\frac{1}{N_{sv}} \\sum_{x_i \\in S_v} (y_i - w^T x_i)\n",
    "    \\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "# Problem data:\n",
    "m = 30   # number of constraints\n",
    "n = 20   # number of variables\n",
    "np.random.seed(1)\n",
    "A = np.random.randn(m, n)   # random matrix\n",
    "b = np.random.randn(m)      # random vector\n",
    "\n",
    "# Construct the problem:\n",
    "x = cp.Variable(n)          # lambda_i\n",
    "objective = cp.Minimize(cp.sum_squares(A @ x - b)) # objective\n",
    "constraints = [0 <= x, x <= 1]      # lambda_i >= 0\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "result = prob.solve()\n",
    "# The optimal value for x is stored in `x.value`.\n",
    "print(x.value)\n",
    "# The optimal Lagrange multiplier for a constraint is stored in\n",
    "# `constraint.dual_value`.\n",
    "print(constraints[0].dual_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
